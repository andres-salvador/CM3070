# -*- coding: utf-8 -*-
"""Final Project De-Fake My News.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12fuFweTOxGFh-Dn-_r3y6_8Ewvhpz65p

# University of London
## BSc in Computer Science Final Project
## De-Fake My News

#### By: Andres Salvador

## Dataset

My data set of choice is the ‘Fake and real news’ dataset available from the Kaggle official website (downloaded Feb 24, 2024):
https://www.kaggle.com/datasets/bhavikjikadara/fake-news-detection

This dataset has 2 csv files, True.csv and False.csv

Each csv file has 4 columns:

  1.   title: The title of the article
  2.   text: The text of the article
  3.   subject: The subject of the article
  4.   date: The date that this article was posted at

This dataset was chosen for the clarity and contents of the files.


Categorical Attributes

    subject: News, politics, left-news, Government News, US_News, Middle-east

Continuous Attributes

    date: continuous

# Initial Data Manipulation

## Library imports and file manipulation
"""

# Standard library imports
import os
import re
import shutil
import string
import warnings

# Third-party imports
from google.colab import drive
import matplotlib.pyplot as plt
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import numpy as np
import pandas as pd
from PIL import Image
import seaborn as sns
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, MaxPooling1D, Bidirectional, LSTM
from sklearn.preprocessing import LabelEncoder
from wordcloud import STOPWORDS, ImageColorGenerator, WordCloud

# NLTK downloads
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('vader_lexicon')

# Warnings configuration
warnings.filterwarnings("ignore")

# folder manipulation
dir = '/content/drive/MyDrive/NLP/extracts'

if os.path.exists(dir):
    shutil.rmtree(dir)

!mkdir dir

# Unzipping the files, set to override for easy re-runs of the entire notebook
!unzip -o '/content/drive/MyDrive/NLP/Fake.csv.zip' -d '/content/drive/MyDrive/NLP/extracts'
!unzip -o '/content/drive/MyDrive/NLP/True.csv.zip' -d '/content/drive/MyDrive/NLP/extracts'

# making sure the files are there
for dirname, _, filenames in os.walk('/content/drive/MyDrive/NLP/extracts'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# assigning the files to datasets
fake=pd.read_csv('/content/drive/MyDrive/NLP/extracts/Fake.csv')
real=pd.read_csv('/content/drive/MyDrive/NLP/extracts/True.csv')

"""## Quick look into the data"""

# rows / columns
fake.shape

# rows / columns
real.shape

# quick look
fake.head()

# quick look
real.head()

# Now, the fake news has this subjects and their count is
fake["subject"].value_counts()

# Now, the real news has this subjects and their count is
real["subject"].value_counts()

# How many different dates are there in the fake dataset
fake["date"].value_counts()

# How many different dates are there in the real dataset
real["date"].value_counts()

"""## Setting up Fake news"""

# Lets add a label to the fake datasets, with 0 (as in how much we trust that row)
fake["label"]=0

"""Do we need to clean the dataset?"""

# Of the fake dataset, how many rows have null values?
fake.isnull().sum()

"""No need to clean the fake data! But if needed to clean this dataset, we would have eliminated the empty rows, since this is text we cant replace the contents with mid/median values"""

# Another quick look into the fake news, now with the label=0
fake.head()

"""## Setting up Real News"""

# Now lets work on the real dataset, lets add that label
real["label"]=1

"""Do we need to clean the dataset?"""

# Of the real dataset, how many rows have null values?
real.isnull().sum()

"""No need to clean the real data! But if needed to clean this dataset, we would have eliminated the empty rows, since this is text we cant replace the contents with mid/median values"""

# Another quick look into the real news, now with the label=1
real.head()

"""## Individual Word cloud visual representation"""

# Now lets do it for the whole fake dataset
text_fake_all = " ".join(review for review in fake.text)

# The text might be too long to display, so lets just get the word count
print("The word count for all the true news text is: ", len(text_fake_all.split()))

# Lets generate the work cloud
wordcloud = WordCloud().generate(text_fake_all)

# Lets display the word cloud
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Now lets do it for the whole real dataset
text_real_all = " ".join(review for review in real.text)

# The text might be too long to display, so lets just get the word count
print("The word count for all the true news text is: ", len(text_real_all.split()))

# Lets generate the work cloud
wordcloud = WordCloud().generate(text_real_all)

# Lets display the word cloud
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""## Unifying the datasets"""

# Now lets join the datasets into 1
data=pd.concat([fake,real],ignore_index=True)
data.head()

# Lets take a quick look at the joined dataset
data.head()

# The shape should only change in the rows
data.shape

# Lets describe the dataset to get a better overview of the dataset
data.describe(include = 'all')

"""## Unified Dataset Word cloud visual representation"""

# Now lets do it for the whole dataset
text_data = " ".join(review for review in data.text)

# The text might be too long to display, so lets just get the word count
print("The word count for all the news text is: ", len(text_data.split()))

# Lets generate the work cloud
wordcloud = WordCloud().generate(text_data)

# Lets display the word cloud
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""# Feature engineering

## Unsorting the data
"""

data.head()

data.tail()

"""After joining the datasets (real and fake), lets unsort them"""

data = data.sample(frac=1).reset_index(drop=True)

data.head()

data.tail()

"""## Removing punctuation from the text

By taking out punctuation, you cut down on the extra stuff in the data. This makes it simpler for the algorithms to pay attention to the main words and what they mean.
"""

# Lets remove the puntiation from the text
# from https://stackoverflow.com/questions/53664775/how-to-remove-punctuation-in-python
# and https://www.geeksforgeeks.org/python-remove-punctuation-from-string/

# Procedure to remove punctuation
def clearPunctuation(text: str) -> str:
    incommingText = text.encode("utf8").decode("ascii",'ignore')
    listOfCharactersWithoutPunctuation = [ch for ch in incommingText if ch not in string.punctuation]
    stringWithoutPunctuation = ''.join(listOfCharactersWithoutPunctuation)
    # Remove trailing spaces
    stringWithoutPunctuation = stringWithoutPunctuation.strip()

    return stringWithoutPunctuation

# adding a column to my dataset, a no-punctuation column
data['no_puntuation_text'] = data['text'].apply(clearPunctuation)

"""Taking a look at the original data"""

original_text = data['text']
original_text.head()

"""Taking a look at the cleaned data"""

cleaned_text = data['no_puntuation_text']
cleaned_text.head()

"""## Lowercasing the text"""

# Seeting up the column with lowercase text
data['lowercase_text'] = data['no_puntuation_text'].str.lower()

"""What we had before"""

data['no_puntuation_text'].head()

"""What we have now"""

data['lowercase_text'].head()

"""## Removing the double spaces

The other steps left double spaces in the text column, lets remove them
"""

# taken and modified from: https://stackoverflow.com/questions/1546226/is-there-a-simple-way-to-remove-multiple-spaces-in-a-string
def remove_double_spaces(text):
    return re.sub(r'\s+', ' ', text)

# Lets add another column, now one without doble empty spaces
data['no_double_spaces'] = data['lowercase_text'].apply(remove_double_spaces)

data['lowercase_text'].head()

data['no_double_spaces'].head()

"""## Removing Stopwords

Now, lets remove the stopwords from the text
"""

# taken and modified from: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_sentence)

# lets add another column, now one with no stopwords
data['no_stopwords'] = data['no_double_spaces'].apply(remove_stopwords)

data['no_stopwords'].head()

"""It is clean now!

## Adding a Sentiment analysis to the dataset
"""

# Taken and modified from: https://stackoverflow.com/questions/57803412/applying-sentimentintensityanalyzer-function-on-each-row-of-the-dataframe-prov

# Initialize the Sentiment Intensity Analyzer
sia = SentimentIntensityAnalyzer()

# Function to get the compound sentiment score
def get_sentiment(text):
    return sia.polarity_scores(text)['compound']

# Lets add another column, but now with the sentiment analysis
data['text_sentiment'] = data['no_stopwords'].apply(get_sentiment)

data.head()

"""Interesting, values are between -1 for bad/sad news and +1 for good/happy news

### Sentiment Analysis on Fake News
"""

# Let's get a sample of Real news and test it against a Sentiment analysis
df_filtered_fake = data[data['label'] == 0]

#Lets limit that to 2000 values, just to graph a sample
df_limited_fake = df_filtered_fake.sample(2000)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(df_limited_fake['label'], df_limited_fake['text_sentiment'])

# Adding titles and labels
plt.title('Scatter Plot of label and Sentiment (Fake, Sample 2000 Rows)')
plt.xlabel('Fake')
plt.ylabel('Sentiment (Range -1 (Negative) to 1 (Positive))')

# Show the plot
plt.show()

"""This seems well distributed, no actual skewness."""

plt.hist(df_limited_fake['text_sentiment'], bins=20, edgecolor='black')

plt.title('Distribution of Text Sentiment')
plt.xlabel('Sentiment Value')
plt.ylabel('Frequency')
plt.show()

"""Now we can see that the sample data leans more to the negative/sad/bad news, but not by much.

Let's examine the whole of the fake news.
"""

# Calculate min, median, max, and mean (average) of Column2
min_val = df_filtered_fake['text_sentiment'].min()
median_val = df_filtered_fake['text_sentiment'].median()
max_val = df_filtered_fake['text_sentiment'].max()
mean_val = df_filtered_fake['text_sentiment'].mean()

(min_val, median_val, max_val, mean_val)

"""This suggests that in this dataset, Fake News tends more to the negative side, but by a little bit.

### Sentiment Analysis on Real News
"""

# Let's get a sample of Real news and test it against a Sentiment analysis
df_filtered_real = data[data['label'] == 1]

#Lets limit that to 200 values, just to graph
df_limited_real = df_filtered_real.sample(2000)

# Plotting
plt.figure(figsize=(10, 6))
plt.scatter(df_limited_real['label'], df_limited_real['text_sentiment'])

# Adding titles and labels
plt.title('Scatter Plot of label and Sentiment (Real, First 100 Rows)')
plt.xlabel('Real or Fake (Real)')
plt.ylabel('Sentiment (Range -1 (Negative) to 1 (Positive))')

# Show the plot
plt.show()

"""This seems well distributed too, no actual skewness."""

plt.hist(df_limited_real['text_sentiment'], bins=20, edgecolor='black')

plt.title('Distribution of Text Sentiment')
plt.xlabel('Sentiment Value')
plt.ylabel('Frequency')
plt.show()

"""Now we can see that the sample data leans more to the positive news, but not by much.

Let's examine whether Real News tends to be more positive overall.
"""

# Calculate min, median, max, and mean (average) of Column2
min_val = df_filtered_real['text_sentiment'].min()
median_val = df_filtered_real['text_sentiment'].median()
max_val = df_filtered_real['text_sentiment'].max()
mean_val = df_filtered_real['text_sentiment'].mean()

(min_val, median_val, max_val, mean_val)

"""This indicates that in this dataset, Real News is generally more positive.

#### Sentiment Analysis Conclusion
Let's note the caveat that this observation is specific to this dataset and should not be used as a standard to compare real versus fake news across the entire news landscape.
That being said, there is no distinct or big enough correlation between the positivity or negativity of content and its classification as real or fake news in this dataset. Real news looked more positive and fake news more negative, but the margins are really small.
However, there is a slight trend: positive news leans towards being positive, while fake news tends to be more negative.

## Adding Character Level Features to the dataset
"""

#Taken and modified from: https://www.geeksforgeeks.org/count-uppercase-lowercase-special-character-numeric-values/

# Feature Extraction Functions

def count_characters(text):
    return len(text)

def count_digits(text):
    return sum(c.isdigit() for c in text)

def count_uppercase(text):
    return sum(c.isupper() for c in text)

# Applying the functions to the DataFrame
data['char_count'] = data['no_stopwords'].apply(count_characters)
data['digit_count'] = data['no_stopwords'].apply(count_digits)
data['uppercase_count'] = data['no_stopwords'].apply(count_uppercase)

# Display the char_count
print(data['char_count'])

# Calculate min, median, max, and mean (average) of char_count
min_val = data['char_count'].min()
median_val = data['char_count'].median()
max_val = data['char_count'].max()
mean_val = data['char_count'].mean()

(min_val, median_val, max_val, mean_val)

# # Display the digit_count
print(data['digit_count'])

# Calculate min, median, max, and mean (average) of Column2
min_val = data['digit_count'].min()
median_val = data['digit_count'].median()
max_val = data['digit_count'].max()
mean_val = data['digit_count'].mean()

(min_val, median_val, max_val, mean_val)

# Display the uppercase_count
print(data['uppercase_count'])

# Calculate min, median, max, and mean (average) of Column2
min_val = data['uppercase_count'].min()
median_val = data['uppercase_count'].median()
max_val = data['uppercase_count'].max()
mean_val = data['uppercase_count'].mean()

(min_val, median_val, max_val, mean_val)

# Plotting
plt.figure(figsize=(10,6))
plt.bar(data.index, data['char_count'])
plt.xlabel('Row Index')
plt.ylabel('Character Count')
plt.title('Character Count in Text Column')
plt.show()

# Limiting the character count to 20000
data['char_count_limited'] = data['char_count'].clip(upper=20000)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(data.index, data['char_count_limited'])
plt.title('Character Count (Limited to 20000) per Entry')
plt.xlabel('Entry Index')
plt.ylabel('Character Count')
plt.show()

"""### CLF on Fake news"""

# Taken and adapted from https://stackoverflow.com/questions/55249360/count-the-number-of-digits-in-a-dataframe-column

# Now just for Fake news
df_filtered_fake = data[data['label'] == 0]
# Applying the functions to the DataFrame
data['char_count'] = df_filtered_fake['no_puntuation_text'].apply(count_characters)
data['digit_count'] = df_filtered_fake['no_puntuation_text'].apply(count_digits)
data['uppercase_count'] = df_filtered_fake['no_puntuation_text'].apply(count_uppercase)

# Display the DataFrame
print(df_filtered_fake['char_count'])

# Plotting
plt.figure(figsize=(10,6))
plt.bar(df_filtered_fake.index, df_filtered_fake['char_count'])
plt.xlabel('Row Index')
plt.ylabel('Character Count')
plt.title('Character Count in Text Column')
plt.show()

# Limiting the character count to 20000
df_filtered_fake['char_count_limited'] = df_filtered_fake['char_count'].clip(upper=20000)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(df_filtered_fake.index, df_filtered_fake['char_count_limited'])
plt.title('Character Count (Limited to 20000) per Entry')
plt.xlabel('Entry Index')
plt.ylabel('Character Count')
plt.show()

"""### CLF on Real news"""

# Now just for Real news
df_filtered_true = data[data['label'] == 1]
# Applying the functions to the DataFrame
df_filtered_true['char_count'] = df_filtered_true['no_stopwords'].apply(count_characters)
df_filtered_true['digit_count'] = df_filtered_true['no_stopwords'].apply(count_digits)
df_filtered_true['uppercase_count'] = df_filtered_true['no_stopwords'].apply(count_uppercase)

# Display the DataFrame
print(df_filtered_true['char_count'])

# Plotting
plt.figure(figsize=(10,6))
plt.bar(df_filtered_true.index, df_filtered_true['char_count'])
plt.xlabel('Row Index')
plt.ylabel('Character Count')
plt.title('Character Count in Text Column')
plt.show()

# Limiting the character count to 20000
df_filtered_true['char_count_limited'] = df_filtered_true['char_count'].clip(upper=20000)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(df_filtered_true.index, df_filtered_true['char_count_limited'])
plt.title('Character Count (Limited to 20000) per Entry')
plt.xlabel('Entry Index')
plt.ylabel('Character Count')
plt.show()

"""#### CLF Conclusion
In this dataset, it seems that Fake News exhibits a more even distribution of character counts.

# NLP Model Training

## Bayes on raw data
"""

# Taken and adapted from: https://www.geeksforgeeks.org/multinomial-naive-bayes/

# Let's get everything ready for using the multinomial Naive Bayes classifier
# (Bayes for text)
# Features are the text of the news
features_raw = data['text']
# Targets are the values 0 for fake and 1 for true
targets_raw = data['label']

# Let's split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(features_raw, targets_raw,
                                                    test_size=0.30,
                                                    random_state=13)

# Since we are want to predict using words, rather than numbers, we need to
# limit the word vocabulary and we need to set the tokenizer to limit the max
# amount of the vocabulary
max_vocabulary = 10000
tokenizer = Tokenizer(num_words=max_vocabulary)
tokenizer.fit_on_texts(X_train)

# Now let's use the tokenizer to turn text into lists
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# This pads the sequences to make them the same length, we need this for processing the data
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)

# Creating the model, in this case Bayes
bayes_model = MultinomialNB()

# Fitting the model with the train data
bayes_model.fit(X_train, y_train)

# Predicting
predicted_bayes_model = bayes_model.predict(X_test)

# Checking values
print("The model Accuracy is: ", accuracy_score(predicted_bayes_model, y_test))
print("The model F1 is: ", f1_score(predicted_bayes_model, y_test))
print("The model Precision is: ", precision_score(predicted_bayes_model, y_test))
print("The model Recall is: ", recall_score(predicted_bayes_model, y_test))

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_bayes_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# Let's fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, y_test, normalize='all')

# Let's plot it (standard 8 by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

"""## Bayes on cleaned data"""

# Let's get everything ready for using the multinomial Naive Bayes classifier
# (Bayes for text)
# Features are the text of the news
features = data['no_puntuation_text']
# Targets are the values 0 for fake and 1 for true
targets = data['label']

# Let's split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(features, targets,
                                                    test_size=0.30,
                                                    random_state=13)

# Since we are want to predict using words, rather than numbers, we need to
# limit the word vocabulary
# and we need to set the tokenizer to limit the max amount of the vocabulary
max_vocabulary = 10000
tokenizer = Tokenizer(num_words=max_vocabulary)
tokenizer.fit_on_texts(X_train)

# Now let's use the tokenizer to turn text into lists
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# This pads the sequences to make them the same length, we need this for
# processing the data
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post',
                                                        maxlen=256)
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post',
                                                       maxlen=256)

# Creating the model, in this case Bayes
bayes_model = MultinomialNB()

# Fitting the model with the train data
bayes_model.fit(X_train, y_train)

# Predicting
predicted_bayes_model = bayes_model.predict(X_test)

# Checking values
print("The model Accuracy is: ", accuracy_score(predicted_bayes_model, y_test))
print("The model F1 is: ", f1_score(predicted_bayes_model, y_test))
print("The model Precision is: ", precision_score(predicted_bayes_model, y_test))
print("The model Recall is: ", recall_score(predicted_bayes_model, y_test))

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_bayes_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# Let's fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, y_test,
                                       normalize='all')

# Let's plot it (standard 8 by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

"""### Conclusion on Bayesian Analysis

It appears that there is a slight improvement in the accuracy of True predictions when using data without punctuation, while the rest of the results remain largely unchanged.

## SimpleRNN

### On raw data
"""

# Taken and adapted from: https://subscription.packtpub.com/book/data/9781788292061/7/ch07lvl1sec57/simple-rnn-with-keras

# Features are the text of the news
features_raw = data['text']
# Targets are the labels
targets_raw = data['label']

# Split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(features_raw, targets_raw, test_size=0.30)

# Basic tokenization and conversion to sequences, limit to 100 words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train[:100])
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Basic padding of sequences
X_train = pad_sequences(X_train, maxlen=100)
X_test = pad_sequences(X_test, maxlen=100)

# Create a model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32),
    tf.keras.layers.SimpleRNN(10),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile and fit the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=1, batch_size=128, validation_split=0.1)

# Evaluate the model
model.evaluate(X_test, y_test)

# Predict and generate binary predictions
predicted = model.predict(X_test)
binary_predictions = [1 if x >= 0.5 else 0 for x in predicted]

# Generate a confusion matrix and plot it
conf_matrix = confusion_matrix(y_test, binary_predictions, normalize='all')
sns.heatmap(conf_matrix, annot=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""### On cleaned data"""

# Features are the text of the news
features_clean = data['no_puntuation_text']
# Targets are labels
targets_clean = data['label']

# Split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(features_clean, targets_clean, test_size=0.30)

# Basic tokenization and conversion to sequences, limit to 100 words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train[:100])
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Basic padding of sequences
X_train = pad_sequences(X_train, maxlen=100)
X_test = pad_sequences(X_test, maxlen=100)

# Create a model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32),
    tf.keras.layers.SimpleRNN(10),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile and fit the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=1, batch_size=128, validation_split=0.1)

# Evaluate the model
model.evaluate(X_test, y_test)

# Predict and generate binary predictions
predicted = model.predict(X_test)
binary_predictions = [1 if x >= 0.5 else 0 for x in predicted]

# Generate a confusion matrix and plot it
conf_matrix = confusion_matrix(y_test, binary_predictions, normalize='all')
sns.heatmap(conf_matrix, annot=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""#### Conclusion on SimpleRNN

On raw data, we get an accuracy of: 97.22%

On cleaned data, we get an accuracy of: 95.64%

In SimpleRNN cleaning the data does affect the accurary of the trained models, in the wrong way!

## GRU

### On raw data
"""

# Taken and adapted from: https://keras.io/api/layers/recurrent_layers/gru/

# Features are the text of the news
features_raw = data['text']
# Targets are the labels
targets_raw = data['label']

# Split the data into train and test datasets (simpler split)
X_train, X_test, y_train, y_test = train_test_split(features_raw, targets_raw, test_size=0.30)

# Basic tokenization and conversion to sequences, limit to 100 words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train[:100])
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Basic padding of sequences
X_train = pad_sequences(X_train, maxlen=100)
X_test = pad_sequences(X_test, maxlen=100)

# Create a model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32),
    tf.keras.layers.GRU(16),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile and fit the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=1, batch_size=128, validation_split=0.1)

# Evaluate the model
model.evaluate(X_test, y_test)

# Predict and generate binary predictions
predicted = model.predict(X_test)
binary_predictions = [1 if x >= 0.5 else 0 for x in predicted]

# Generate a confusion matrix and plot it
conf_matrix = confusion_matrix(y_test, binary_predictions, normalize='all')
sns.heatmap(conf_matrix, annot=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""### On cleaned data"""

# Features are the text of the news
features_clean = data['no_puntuation_text']
# Targets are the labels
targets_clean = data['label']

# Split the data into train and test datasets (simpler split)
X_train, X_test, y_train, y_test = train_test_split(features_clean, targets_clean, test_size=0.30)

# Basic tokenization and conversion to sequences, limit to 100 words
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train[:100])
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Basic padding of sequences
X_train = pad_sequences(X_train, maxlen=100)
X_test = pad_sequences(X_test, maxlen=100)

# Create a model
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, 32),
    tf.keras.layers.GRU(16),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile and fit the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=1, batch_size=128, validation_split=0.1)

# Evaluate the model
model.evaluate(X_test, y_test)

# Predict and generate binary predictions
predicted = model.predict(X_test)
binary_predictions = [1 if x >= 0.5 else 0 for x in predicted]

# Generate a confusion matrix and plot it
conf_matrix = confusion_matrix(y_test, binary_predictions, normalize='all')
sns.heatmap(conf_matrix, annot=True)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""#### Conclusion on GRU

On raw data, we get an accuracy of: 96.99%

On cleaned data, we get an accuracy of: 96.06%

In GRU cleaning the data affect the accurary, it is more accurate to use the raw data!

## LSTM

### On raw data
"""

# Taken and adapted from: https://keras.io/examples/nlp/bidirectional_lstm_imdb/

# features are the text of the news
features_raw = data['text']
# targets are labels
targets_raw = data['label']

# Let's split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(features_raw, targets_raw,
                                                    test_size=0.30, random_state=7)

# Since we want to predict using words rather than numbers, we need to limit the word vocabulary
# and we need to set the tokenizer to limit the max amount of the vocabulary
max_vocabulary = 10000
tokenizer = Tokenizer(num_words=max_vocabulary)
tokenizer.fit_on_texts(X_train)

# Now, let's use the tokenizer to turn text into lists
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# This pads the sequences to make them the same length; we need this for processing the data
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post',
                                                        maxlen=256)
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post',
                                                       maxlen=256)

# Let's create the model now!
model = tf.keras.Sequential([
    # This is the embedding layer, to convert into dense vectors for better input
    tf.keras.layers.Embedding(max_vocabulary, 128),
    # This will process the input in both directions; it helps with work context
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),
    # This is technical, but it applies a non-linear transformation to the inputs
    # using the ReLU activation function
    tf.keras.layers.Dense(64, activation='relu'),
    # This prevents overfitting by setting a fraction of the inputs as 0
    tf.keras.layers.Dropout(0.5),
    # This predicts the final value
    tf.keras.layers.Dense(1)
])

# Let's print a summary of the model
model.summary()

# Let's compile the model
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-3),
              metrics=['accuracy'])

# Let's fit the model
model_fitter = model.fit(X_train, y_train, epochs=1, validation_split=0.1,
                         batch_size=30, shuffle=True)


# Let's evaluate it
model.evaluate(X_test, y_test)

# Lets predict something
predicted_LSTM_model = model.predict(X_test)

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_LSTM_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# lets fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, y_test,
                                       normalize='all')

# Lets plot it (standard eight by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""### On cleaned data"""

# Let's get everything ready for using the multinomial Naive Bayes classifier (Bayes for text)
# features are the text of the news
features_raw = data['no_puntuation_text']
# targets are the values 0 for fake and 1 for true
targets_raw = data['label']

# Let's split the data into train and test datasets
X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.30, random_state=7)

# Since we want to predict using words rather than numbers, we need to limit the word vocabulary
# and we need to set the tokenizer to limit the max amount of the vocabulary
max_vocabulary = 10000
tokenizer = Tokenizer(num_words=max_vocabulary)
tokenizer.fit_on_texts(X_train)

# Now, let's use the tokenizer to turn text into lists
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# This pads the sequences to make them the same length; we need this for processing the data
X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)
X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)

# Features are the text of the news
features = data['no_puntuation_text']
# Targets are the values 0 for fake and 1 for true
targets = data['label']

# Let's create the model now!
model = tf.keras.Sequential([
    # This is the embedding layer, to convert into dense vectors for better input
    tf.keras.layers.Embedding(max_vocabulary, 128),
    # This will process the input in both directions; it helps with work context
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),
    # This is technical, but it applies a non-linear transformation to the inputs
    # using the ReLU activation function
    tf.keras.layers.Dense(64, activation='relu'),
    # This prevents overfitting by setting a fraction of the inputs as 0
    tf.keras.layers.Dropout(0.5),
    # This predicts the final value
    tf.keras.layers.Dense(1)
])

# Let's print a summary of the model
model.summary()

# Let's compile the model
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              optimizer=tf.keras.optimizers.Adam(1e-3),
              metrics=['accuracy'])

# Let's fit the model
model_fitter = model.fit(X_train, y_train, epochs=1, validation_split=0.1,
                         batch_size=30, shuffle=True)


# Let's evaluate it
model.evaluate(X_test, y_test)

# Lets predict something
predicted_LSTM_model = model.predict(X_test)

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_LSTM_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# lets fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, y_test,
                                       normalize='all')

# Lets plot it (standard eight by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""#### Conclusion on LSTM

On raw data, we get an accuracy of: 99.32%

On cleaned data, we get an accuracy of: 98.95%

In LSTM cleaning the data affects the accurary, it is more accurate to use the raw data again, this can't be a coincidence!

## CNN

### On raw data
"""

# Taken and adapted from: https://keras.io/api/layers/convolution_layers/convolution1d/

# Data Preparation
X = data['text']
y = data['label']

# Label encoding
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Text Vectorization
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)
X_pad = pad_sequences(X_seq, maxlen=100)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded,
                                                    test_size=0.2,
                                                    random_state=42)

# Building the CNN Model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=50, input_length=100))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' if you have more than two classes

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

# Let's evaluate it
model.evaluate(X_test, y_test)

# Lets predict something
predicted_CNN_model = model.predict(X_test)

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_CNN_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# lets fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, y_test,
                                       normalize='all')

# Lets plot it (standard eight by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""### On cleaned data"""

# Data Preparation
X = data['no_stopwords']
y = data['label']

# Label encoding
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Text Vectorization
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X)
X_seq = tokenizer.texts_to_sequences(X)
X_pad = pad_sequences(X_seq, maxlen=100)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X_pad, y_encoded,
                                                    test_size=0.2, random_state=42)

# Building the CNN Model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=50, input_length=100))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(10, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Use 'softmax' if you have more than two classes

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))

# Let's evaluate it
model.evaluate(X_test, y_test)

# Lets predict something
predicted_CNN_model = model.predict(X_test)

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_CNN_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# lets fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, y_test,
                                       normalize='all')

# Lets plot it (standard eight by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""#### Conclusion on CNN

On raw data, we get an accuracy of: 98.86%

On cleaned data, we get an accuracy of: 98.24%

In CNN cleaning the data affects the accurary, it is more accurate to use the raw data yet again!

## Now, CNN + bidirectional LSTM

### On raw data
"""

# Taken and adapted from: https://stackoverflow.com/questions/64150587/combining-cnn-and-bidirectional-lstm

# Parameters
vocab_size = 10000
embedding_dim = 64
max_length = 50
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

# Tokenize the data
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(data['text'])
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(data['text'])
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type,
                       truncating=trunc_type)

# Split data into training and testing (example split)
train_size = int(len(data) * 0.8)
train_sequences = padded[0:train_size]
train_labels = data['label'][0:train_size]
test_sequences = padded[train_size:]
test_labels = data['label'][train_size:]

# Building the model
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Conv1D(64, 5, activation='relu'),
    MaxPooling1D(pool_size=4),
    Bidirectional(LSTM(64)),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam',
              metrics=['accuracy'])

# Train the model
model.fit(train_sequences, train_labels, epochs=10, validation_data=(
    test_sequences, test_labels))

# Let's evaluate it
model.evaluate(test_sequences, test_labels)

# Lets predict something
predicted_CNN_LSTM_model = model.predict(test_sequences)

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_CNN_LSTM_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# lets fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, test_labels,
                                       normalize='all')

# Lets plot it (standard eight by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

# Evaluate the model
loss, accuracy = model.evaluate(test_sequences, test_labels)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""### On cleaned data"""

# Parameters
vocab_size = 10000
embedding_dim = 64
max_length = 50
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"

# Tokenize the data
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(data['no_stopwords'])
word_index = tokenizer.word_index
sequences = tokenizer.texts_to_sequences(data['no_stopwords'])
padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type,
                       truncating=trunc_type)

# Split data into training and testing (example split)
train_size = int(len(data) * 0.8)
train_sequences = padded[0:train_size]
train_labels = data['label'][0:train_size]
test_sequences = padded[train_size:]
test_labels = data['label'][train_size:]

# Building the model
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Conv1D(64, 5, activation='relu'),
    MaxPooling1D(pool_size=4),
    Bidirectional(LSTM(64)),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam',
              metrics=['accuracy'])

# Train the model
model.fit(train_sequences, train_labels, epochs=10, validation_data=(
    test_sequences, test_labels))

# Let's evaluate it
model.evaluate(test_sequences, test_labels)

# Lets predict something
predicted_CNN_LSTM_model = model.predict(test_sequences)

# Now let's try to graph that model

# This is where I'll store the predictions
binary_predictions = []

# Let's place the predicted values into buckets and then into my storage
for i in predicted_CNN_LSTM_model:
    # If it was higher/equal to 0.5, it's a 1 (true)
    if i >= 0.5:
        binary_predictions.append(1)
    # It's false
    else:
        binary_predictions.append(0)

# lets fill out the confusion matrix with values
matrix_of_confusion = confusion_matrix(binary_predictions, test_labels,
                                       normalize='all')

# Lets plot it (standard eight by 5)
plt.figure(figsize=(8, 5))

matrix_graph = plt.subplot()
# Adding a heat map
sns.heatmap(matrix_of_confusion, annot=True, ax = matrix_graph)
# Labeling and prettying it up
matrix_graph.set_xlabel('Predicted Labels', size=18)
matrix_graph.set_ylabel('True Labels', size=18)
matrix_graph.set_title('Confusion Matrix', size=25)
matrix_graph.xaxis.set_ticklabels([0,1], size=10)
matrix_graph.yaxis.set_ticklabels([0,1], size=10)

# Evaluate the model
loss, accuracy = model.evaluate(test_sequences, test_labels)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

"""#### Conclusion on CNN + bidirectional LSTM

On raw data, we get an accuracy of: 99.84%

On cleaned data, we get an accuracy of: 99.89%

Now this makes more sense, with CNN + bidirectional LSTM cleaning the data affects the accurary, it is better to use clean data to get more accuracy.

99.84% is the best I've seen!

# Summary and conclusions

## Algoritms
Bayes was our baseline, simple and a low bar.

Simple RNN was a high bar, we selected it because it was mentioned more than once on the research papers.

GRU was a clear next bar, better than SimpleRNN but requiered more computing power.

LSTM, CNN and CNN+Bidirectional LSTM, was something we also read from the research papers, adding two layers was something new and required the most computational power i have ever done (this requiered that I pay for credits on Google Colab, otherwise it woudl have taken days to execute on the free version)

## Conclusion
Better algorithms are heavier and requiere more computeing power, researching them and implementing them is not easy and the worse part is modifying the parameters to be able to run in hours and not days.

I was able to run everything in a few hours only becuase I was able to pay for Google Colab.

In my computer, it would have taken days (I have an old computer Intel i3 from 5 years ago)

Reaching over 99% was a dream, I took weeks of testing each algoritm to be able to run them in my computer, it was a shocking experience runing them in Google Colab after paying, it took minutes instead of hours and days.

I learned that in order to trully be able to do this, you need good computing power, everything else just consumes your patience and life.
"""

